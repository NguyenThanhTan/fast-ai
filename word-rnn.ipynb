{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('corpus length:', 600901)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "words = list(re.split(\"[, \\-!?:\\n]+\",text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "counter = collections.Counter(words)\n",
    "d = dict()\n",
    "chars = []\n",
    "for i, (word, freq) in enumerate(counter.most_common()):\n",
    "    chars.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'of', 'and', 'to', 'in', 'is', 'a', 'that', 'as', 'it']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total words:', 13183)\n"
     ]
    }
   ],
   "source": [
    "print(\"total words:\", len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_char[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = [char_indices[c] for c in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5542, 552, 7, 144, 5]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total sentences:', 101645)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "maxlen = 40\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(idx)-maxlen+1):\n",
    "    sentences.append(idx[i:i+maxlen])\n",
    "    next_chars.append(idx[i+1:i+maxlen+1])\n",
    "\n",
    "print('total sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = np.concatenate([[np.array(o)] for o in sentences[:-2]])\n",
    "next_chars = np.concatenate([[np.array(o)] for o in next_chars[:-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((101643, 40), (101643, 40))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.shape, next_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((101643, 40), (101643, 40))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_fac = 50\n",
    "vocab_size = 5000\n",
    "\n",
    "sentences = np.where(sentences < vocab_size-1, sentences, vocab_size-1)\n",
    "next_chars = np.where(next_chars < vocab_size-1,next_chars, vocab_size-1)\n",
    "sentences.shape, next_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import bcolz\n",
    "import pickle \n",
    "\n",
    "def get_glove_dataset(dataset):\n",
    "    \"\"\"Download the requested glove dataset from files.fast.ai\n",
    "    and return a location that can be passed to load_vectors.\n",
    "    \"\"\"\n",
    "    # see wordvectors.ipynb for info on how these files were\n",
    "    # generated from the original glove data.\n",
    "    md5sums = {'6B.50d': '8e1557d1228decbda7db6dfd81cd9909',\n",
    "               '6B.100d': 'c92dbbeacde2b0384a43014885a60b2c',\n",
    "               '6B.200d': 'af271b46c04b0b2e41a84d8cd806178d',\n",
    "               '6B.300d': '30290210376887dcc6d0a5a6374d8255'}\n",
    "    glove_path = os.path.abspath('data/glove/results')\n",
    "    %mkdir -p $glove_path\n",
    "    return get_file(dataset,\n",
    "                    'http://files.fast.ai/models/glove/' + dataset + '.tgz',\n",
    "                    cache_subdir=glove_path,\n",
    "                    md5_hash=md5sums.get(dataset, None),\n",
    "                    untar=True)\n",
    "\n",
    "def load_vectors(loc):\n",
    "    return (bcolz.open(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untaring file...\n"
     ]
    }
   ],
   "source": [
    "vecs, words, wordidx = load_vectors(get_glove_dataset('6B.50d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def create_emb():\n",
    "    n_fact = vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "\n",
    "    for i in range(1,len(emb)):\n",
    "        word = indices_char[i]\n",
    "        if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "            src_idx = char_indices[word]\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            emb[i] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  4.48033338e-03,   7.89399991e-02,  -5.63300004e-02,\n",
       "          1.36503329e-01,   2.12706665e-01,   1.59030000e-01,\n",
       "         -1.42839998e-01,  -1.85470005e-01,  -1.21333331e-01,\n",
       "         -7.97933340e-02,   4.33366646e-02,  -2.12446675e-02,\n",
       "         -1.31916662e-01,  -1.60540005e-01,   7.76366691e-02,\n",
       "          3.00669993e-02,  -4.44133331e-02,   2.62130002e-02,\n",
       "         -1.38779998e-01,  -5.14266690e-02,   3.35600004e-02,\n",
       "          1.62969997e-01,   1.04086667e-01,  -4.17333345e-02,\n",
       "         -1.25040002e-02,  -5.05966663e-01,   4.20400004e-02,\n",
       "         -8.14000020e-03,  -1.43203338e-02,  -9.45033332e-02,\n",
       "          1.18053333e+00,  -3.98533344e-02,  -4.84433336e-03,\n",
       "         -4.99666681e-02,   7.28799999e-02,  -1.11373335e-01,\n",
       "         -4.62400019e-02,   1.06020004e-01,   2.34526674e-01,\n",
       "          1.49526666e-01,  -2.67539993e-02,   2.10009992e-01,\n",
       "          1.07036670e-01,  -1.55883332e-01,   7.59533346e-02,\n",
       "          1.20113333e-01,  -1.26059999e-01,  -1.88856661e-01,\n",
       "          1.48970000e-02,   1.01306667e-01],\n",
       "       [  5.05466660e-02,   1.00590001e-01,  -5.58766673e-02,\n",
       "          5.89466691e-02,   1.05729997e-01,   1.13243332e-01,\n",
       "         -1.44926667e-01,  -1.03620003e-01,  -1.49996668e-01,\n",
       "         -9.82866685e-02,   5.53599993e-02,   3.98766672e-02,\n",
       "         -1.37760003e-01,  -1.41176671e-01,   1.99560006e-01,\n",
       "          9.60833331e-02,  -3.84899999e-02,  -1.39493334e-02,\n",
       "         -2.26629992e-01,  -8.35433304e-02,   6.15733316e-02,\n",
       "          2.89586658e-02,   1.55273338e-01,   5.01166657e-03,\n",
       "          1.44913333e-02,  -4.89033341e-01,  -1.01280004e-01,\n",
       "         -7.81366664e-03,   1.01963331e-01,  -7.26166666e-02,\n",
       "          1.24866668e+00,   1.40946669e-03,  -6.14533325e-02,\n",
       "         -1.54029995e-01,   3.27763334e-02,  -3.96900003e-02,\n",
       "          7.97299991e-02,   3.86999995e-02,   1.39016668e-01,\n",
       "          1.89210003e-02,  -2.12270000e-05,   2.29956657e-02,\n",
       "          2.93130005e-02,  -3.42833325e-02,  -4.64366674e-02,\n",
       "          7.43800004e-02,  -2.69343331e-02,  -1.18839999e-01,\n",
       "          5.47099983e-03,   3.40533331e-02],\n",
       "       [  2.36176670e-01,   1.90293332e-01,  -1.57199999e-01,\n",
       "          6.01600011e-02,   1.81496660e-01,   2.42009997e-01,\n",
       "          6.05233312e-02,  -1.74643338e-01,   3.46033325e-02,\n",
       "         -5.85533331e-02,   2.62839993e-02,  -1.20719999e-01,\n",
       "         -3.94299999e-02,  -2.77786672e-01,   3.97233342e-02,\n",
       "         -5.53500007e-02,   2.05183327e-02,  -4.23966659e-03,\n",
       "         -1.88743333e-01,   4.53866677e-03,   7.61700024e-02,\n",
       "         -4.79866664e-02,  -2.25163326e-02,  -1.27190004e-01,\n",
       "         -7.89933354e-02,  -5.67899982e-01,  -2.88973331e-01,\n",
       "         -8.90133381e-02,  -8.62999956e-02,   5.88999987e-02,\n",
       "          1.28919999e+00,  -5.37666678e-02,  -4.42433357e-02,\n",
       "         -2.29603330e-01,   6.14800006e-02,   1.74879997e-03,\n",
       "         -1.12913330e-01,  -2.63186668e-02,   8.06166679e-02,\n",
       "          1.21920000e-01,  -1.15756671e-01,   9.49433347e-02,\n",
       "          2.52309988e-02,  -2.07260003e-02,  -1.29960001e-01,\n",
       "          7.63399998e-02,  -7.20566660e-02,  -7.52066672e-02,\n",
       "         -3.13060010e-02,  -2.67916660e-01],\n",
       "       [  2.26823330e-01,  -1.30876663e-02,   1.00620002e-01,\n",
       "         -5.93066663e-02,   1.43206666e-01,   1.07486670e-02,\n",
       "         -1.37920002e-01,   4.40933357e-02,  -9.94899968e-02,\n",
       "         -2.84176668e-02,   5.70599983e-02,   7.47299989e-02,\n",
       "         -3.34866668e-02,  -1.45509998e-01,   1.11393332e-01,\n",
       "          2.26153334e-01,   1.90680002e-02,  -1.14826669e-01,\n",
       "         -1.42616669e-01,  -1.44249996e-01,   1.86543326e-01,\n",
       "          3.34399988e-02,   6.22566690e-02,  -8.95133317e-02,\n",
       "          1.24446663e-02,  -6.97733323e-01,   7.39033322e-02,\n",
       "         -1.32893334e-01,   6.97066685e-02,  -1.85750008e-01,\n",
       "          1.29420002e+00,   1.58220003e-01,  -3.18859994e-01,\n",
       "         -1.25960002e-01,   6.95633342e-02,  -1.09173338e-01,\n",
       "          4.25033321e-02,   2.94529994e-02,   5.45033316e-02,\n",
       "         -7.21133351e-02,  -3.14583331e-02,   6.10800025e-03,\n",
       "          7.01600015e-02,  -1.02933335e-02,  -6.57399992e-02,\n",
       "          2.74263322e-02,  -3.14466655e-02,  -2.44323338e-02,\n",
       "         -2.15663339e-02,  -8.68133307e-02]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, TimeDistributed, Activation\n",
    "model=Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=maxlen,weights=[emb]),\n",
    "        LSTM(512, input_dim=n_fac,return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        Dropout(0.2),\n",
    "        LSTM(256, return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(vocab_size)),\n",
    "        Activation('softmax')\n",
    "    ])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'of', 'and', 'to', 'in']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "def print_example():\n",
    "    seed_string = []\n",
    "    for i in range(50):\n",
    "        seed_string.append(choice(chars[:500]))\n",
    "\n",
    "    for i in range(320):\n",
    "        x=np.array([char_indices[c] for c in seed_string[-40:]])[np.newaxis,:]\n",
    "        preds = model.predict(x, verbose=0)[0][-1]\n",
    "        preds = preds/np.sum(preds)\n",
    "        next_char = choice(chars[:5000], p=preds)\n",
    "        seed_string.append(next_char)\n",
    "    print(seed_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "101643/101643 [==============================] - 3390s - loss: 6.0387  \n",
      "Epoch 2/2\n",
      "101643/101643 [==============================] - 3445s - loss: 5.8205  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc44f5924d0>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sentences, np.expand_dims(next_chars,-1), batch_size=64, nb_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "101643/101643 [==============================] - 3481s - loss: 5.7040  \n",
      "Epoch 2/2\n",
      "101643/101643 [==============================] - 3508s - loss: 5.5846  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc451182a90>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sentences, np.expand_dims(next_chars,-1), batch_size=64, nb_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virtue', 'conditions', 'self', 'type', 'this', 'learn', 'extent', 'person', 'no', 'pain', 'taste', 'hard', 'however', 'am', 'under', 'him.', 'a', 'morals', 'living', 'reason', 'strange', 'hence', 'much', 'had', 'law', 'cause', 'he', 'this', 'course', 'purpose', 'without', 'such', 'know', 'see', 'work', 'shall', 'day', 'pleasure', 'whom', 'circumstances', '=the', 'nothing', 'music', 'evil', 'we', 'eyes', 'young', 'time', 'moral', 'until', 'he', 'are', 'have', 'with', 'the', 'acquired', 'backward', 'plato', 'frittered', 'of', 'world', 'to', 'dread', 'man', 'beneath', 'him', 'is', 'not', 'so', 'the', 'believe', 'of', 'the', 'desire', 'the', 'frittered', 'frittered', 'as', 'present', 'it.', 'if', 'the', 'finally', 'or', 'dreadful', 'than', 'among', 'frittered', 'by', 'out', 'against', 'let', 'two', 'frittered', 'instinct', 'to', 'the', 'wish', 'in', 'blessedness', 'for', 'badly', 'frittered', 'beyond', 'has', 'romantic', 'nature;', 'of', 'way', 'to', 'frittered', 'that', '=the', 'noble', '\"thou', 'saint', 'draw', 'whom', 'imposes', 'even', 'spectacle', 'acknowledge', 'frittered', 'but', 'did', 'life', 'rid', 'it', 'englishmen', 'of', 'others', 'in', 'of', 'regard', 'affords', 'up', 'my', 'experiences.', 'grows', 'ever', 'far', 'rather', 'resolutely', 'has', 'bliss', 'to', 'the', 'former', 'he', 'frittered', 'and', 'wholly', 'profound', 'dance', 'for', 'a', 'morality', 'none', 'indeed', 'naturally', 'to', 'no', 'coarseness', 'must', 'mean', 'regard', 'minds', 'sounds', 'and', 'you', 'care', 'to', 'a', 'frittered', 'additional', 'philosophy', 'chapter', 'significance', 'even', 'and', 'voice', 'but', 'of', 'general', 'advise', 'frittered', 'among', 'can', 'the', 'disease', 'that', 'to', 'a', 'wether', 'make', 'one', 'profound', 'frittered', 'it', 'how', 'with', 'others', 'must', 'metaphysics', 'and', 'frittered', 'this', 'frittered', 'heavy', 'purpose', 'of', 'recognize', 'that', 'the', 'reward', 'frittered', 'together', 'or', 'must', 'at', 'his', 'explanations', 'as', 'the', 'strong', 'magic', 'order', 'it', 'as', 'granted', 'frittered', 'belongs', 'and', 'also', 'humanity', 'as', 'constant', 'to)', 'which', 'delusions', 'that', 'teeth', 'valuations', 'often', 'regarded', 'to', 'frittered', 'reveal', 'of', 'frittered', 'as', 'alone', 'as', 'a', 'may', 'frittered', 'entangled', 'while', 'some', 'frittered', 'seeks', 'imparts', 'danger', 'with', 'will;', 'for', 'of', 'sight', 'of', 'its', 'the', 'neutralize', '\"', 'for', 'a', 'innate', 'their', 'even.', 'frittered', 'the', 'frittered', 'of', 'as', 'equally', 'else', 'the', 'frittered', 'time', 'originated', 'by', 'frittered', 'that', 'and', 'the', 'word', 'and', 'this', 'and', 'and', 'foundation', 'has', 'a', 'same', 'the', 'apprehending', 'manifold', 'estimates', 'of', 'nature', 'what', 'he', 'cannot', 'play', 'in', 'an', 'point', 'one', 'the', 'moral', 'religion', 'every', 'frittered', 'be', 'irresponsibility', 'what', 'need', 'that', 'interpretation', 'were', 'involved', 'much', 'to', 'us', 'may', 'possible', 'to', 'result', 'may', 'whom', 'the', 'frittered', 'frittered', 'that', 'should', 'dinge)', 'not', 'cosmical', 'plebeian', 'a', 'sense', 'which', 'that', 'beautiful', 'in', 'we', 'not', 'slave', 'a', 'beloved.', 'may', 'god', 'comes', 'once', 'when', 'the', 'attractive', 'writings', 'or', 'the', 'transition']\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('data/word_rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
