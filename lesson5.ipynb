{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'data/imdb/models/'\n",
    "%mkdir -p $model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to look at the IMDB dataset, which contains movie reviews from IMDB, along with their sentiment. Keras comes with some helpers for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: Unable to get the number of gpus available: unknown error)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.pkl\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "idx = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## idx_arr = sorted(idx, key=idx.get)\n",
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and this is the mapping from id to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx2word = {v: k for k, v in idx.iteritems()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the reviews using code copied from keras.datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_full.pkl\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "import pickle\n",
    "path = get_file('imdb_full.pkl',\n",
    "                origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "f = open(path, 'rb')\n",
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the 1st review. As you see, the words have been replaced by ids. The ids can be looked up in idx2word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23022, 309, 6, 3, 1069, 209, 9, 2175, 30, 1, 169, 55, 14, 46, 82, 5869, 41, 393, 110, 138, 14, 5359, 58, 4477, 150, 8, 1, 5032, 5948, 482, 69, 5, 261, 12, 23022, 73935, 2003, 6, 73, 2436, 5, 632, 71, 6, 5359, 1, 25279, 5, 2004, 10471, 1, 5941, 1534, 34, 67, 64, 205, 140, 65, 1232, 63526, 21145, 1, 49265, 4, 1, 223, 901, 29, 3024, 69, 4, 1, 5863, 10, 694, 2, 65, 1534, 51, 10, 216, 1, 387, 8, 60, 3, 1472, 3724, 802, 5, 3521, 177, 1, 393, 10, 1238, 14030, 30, 309, 3, 353, 344, 2989, 143, 130, 5, 7804, 28, 4, 126, 5359, 1472, 2375, 5, 23022, 309, 10, 532, 12, 108, 1470, 4, 58, 556, 101, 12, 23022, 309, 6, 227, 4187, 48, 3, 2237, 12, 9, 215'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(map(str, x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first word of the first review is 23022. Let's see what that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[23022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the whole review, mapped from ids to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell high's satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers' pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i'm here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isn't\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[o] for o in x_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are 1 for positive, 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce vocab size by setting rare words to max index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocab_size = 5000\n",
    "\n",
    "trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\n",
    "test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at distribution of lengths of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2493, 10, 237.71364)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(map(len, trn))\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad (with zero) or truncate each sentence to make consistent length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "seq_len = 500\n",
    "\n",
    "trn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\n",
    "test = sequence.pad_sequences(test, maxlen=seq_len, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in nice rectangular matrices that can be passed to ML algorithms. Reviews shorter than 500 words are pre-padded with zeros, those greater are truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,   10,  432,    2,  216,\n",
       "         11,   17,  233,  311,  100,  109, 4999,    5,   31,    3,  168,\n",
       "        366,    4, 1920,  634,  971,   12,   10,   13, 4999,    5,   64,\n",
       "          9,   85,   36,   48,   10,  694,    4, 4999, 4999,   26,   13,\n",
       "         61,  499,    5,   78,  209,   10,   13,  352, 4999,  253,    1,\n",
       "        106,    4, 3270, 4999,   52,   70,    2, 1839, 4999,  253, 1019,\n",
       "       4999,   16,  138, 4999,    1, 1910,    4,    3,   49,   17,    6,\n",
       "         12,    9,   67, 2885,   16,  260, 1435,   11,   28,  119,  615,\n",
       "         12,    1,  433,  747,   60,   13, 2959,   43,   13, 3080,   31,\n",
       "       2126,  312,    1,   83,  317,    4,    1,   17,    2,   68, 1678,\n",
       "          5, 1671,  312,    1,  330,  317,  134, 4999,    1,  747,   10,\n",
       "         21,   61,  216,  108,  369,    8, 1671,   18,  108,  365, 2068,\n",
       "        346,   14,   70,  266, 2721,   21,    5,  384,  256,   64,   95,\n",
       "       2575,   11,   17,   13,   84,    2,   10, 1464,   12,   22,  137,\n",
       "         64,    9,  156,   22, 1916], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trn.shape\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Single hidden layer NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The simplest model that tends to give reasonable results is a single hidden layer net. So let's try that. Note that we can't expect to get any useful results by feeding word ids directly into a neural net - so instead we use an embedding to replace them with a vector of 32 (initially random) floats for each word in the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, TimeDistributed, Activation, Flatten\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len, dropout=0.3),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_15 (Embedding)         (None, 500, 32)       160000      embedding_input_13[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)             (None, 500, 32)       0           embedding_15[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)             (None, 16000)         0           dropout_23[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 100)           1600100     flatten_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)             (None, 100)           0           dense_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 1)             101         dropout_24[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,760,201\n",
      "Trainable params: 1,760,201\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 39s - loss: 0.5509 - acc: 0.6889 - val_loss: 0.3529 - val_acc: 0.8418\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 39s - loss: 0.3060 - acc: 0.8726 - val_loss: 0.2939 - val_acc: 0.8773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f40072d50>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The [stanford paper](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf) that this dataset is from cites a state of the art accuracy (without unlabelled data) of 0.883. So we're short of that, but on the right track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Single conv layer with max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A CNN is likely to work better, since it's designed to take advantage of ordered data. We'll need to use a 1D CNN, since a sequence of words is 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Convolution1D, MaxPooling1D, Activation\n",
    "\n",
    "conv1 = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len, dropout=0.2),\n",
    "    Dropout(0.2),\n",
    "    Convolution1D(64, 5, border_mode='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "#    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_14 (Embedding)         (None, 500, 32)       160000      embedding_input_12[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)             (None, 500, 32)       0           embedding_14[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_8 (Convolution1D)  (None, 500, 64)       10304       dropout_21[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)             (None, 500, 64)       0           convolution1d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_8 (MaxPooling1D)    (None, 250, 64)       0           dropout_22[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)             (None, 16000)         0           maxpooling1d_8[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 100)           1600100     flatten_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_23 (Dense)                 (None, 1)             101         dense_22[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 1,770,505\n",
      "Trainable params: 1,770,505\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "conv1.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "conv1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 170s - loss: 0.4266 - acc: 0.7755 - val_loss: 0.2693 - val_acc: 0.8906\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 181s - loss: 0.2623 - acc: 0.8888 - val_loss: 0.2591 - val_acc: 0.8940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f40eccd10>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That's well past the Stanford paper's accuracy - another win for CNNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1.save_weights(model_path + 'conv1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1.load_weights(model_path + 'conv1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pre-trained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You may want to look at wordvectors.ipynb before moving on.\n",
    "\n",
    "In this section, we replicate the previous CNN, but using pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove_dataset(dataset):\n",
    "    \"\"\"Download the requested glove dataset from files.fast.ai\n",
    "    and return a location that can be passed to load_vectors.\n",
    "    \"\"\"\n",
    "    # see wordvectors.ipynb for info on how these files were\n",
    "    # generated from the original glove data.\n",
    "    md5sums = {'6B.50d': '8e1557d1228decbda7db6dfd81cd9909',\n",
    "               '6B.100d': 'c92dbbeacde2b0384a43014885a60b2c',\n",
    "               '6B.200d': 'af271b46c04b0b2e41a84d8cd806178d',\n",
    "               '6B.300d': '30290210376887dcc6d0a5a6374d8255'}\n",
    "    glove_path = os.path.abspath('data/glove/results')\n",
    "    %mkdir -p $glove_path\n",
    "    return get_file(dataset,\n",
    "                    'http://files.fast.ai/models/glove/' + dataset + '.tgz',\n",
    "                    cache_subdir=glove_path,\n",
    "                    md5_hash=md5sums.get(dataset, None),\n",
    "                    untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import bcolz\n",
    "def load_vectors(loc):\n",
    "    return (bcolz.open(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untaring file...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs, words, wordidx = load_vectors(get_glove_dataset('6B.50d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carray((400000, 50), float32)\n",
       "  nbytes := 76.29 MB; cbytes := 75.14 MB; ratio: 1.02\n",
       "  cparams := cparams(clevel=5, shuffle=1, cname='lz4', quantize=0)\n",
       "  chunklen := 2621; chunksize: 524200; blocksize: 32768\n",
       "  rootdir := '/home/ubuntu/fast-ai/data/glove/results/6B.50d.dat'\n",
       "  mode    := 'a'\n",
       "[[ 0.41800001  0.24968    -0.41242    ..., -0.18411    -0.11514    -0.78580999]\n",
       " [ 0.013441    0.23682    -0.16899    ..., -0.56656998  0.044691    0.30392   ]\n",
       " [ 0.15164     0.30177    -0.16763    ..., -0.35652     0.016413    0.10216   ]\n",
       " ..., \n",
       " [-0.51181     0.058706    1.09130001 ..., -0.25003001 -1.125       1.58630002]\n",
       " [-0.75897998 -0.47426     0.47369999 ...,  0.78953999 -0.014116\n",
       "   0.64480001]\n",
       " [ 0.072617   -0.51393002  0.47279999 ..., -0.18907    -0.59021002\n",
       "   0.55558997]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordidx #dict\n",
    "words #list\n",
    "vecs #np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The glove word ids and imdb word ids use different indexes. So we create a simple function that creates an embedding matrix using the indexes from imdb, and the embeddings from glove (where they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def create_emb():\n",
    "    n_fact = vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "\n",
    "    for i in range(1,len(emb)):\n",
    "        word = idx2word[i]\n",
    "        if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "            src_idx = wordidx[word]\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            emb[i] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  1.39333338e-01,   8.32266659e-02,  -1.37473335e-01,\n",
       "          4.05666654e-02,   1.15090003e-01,  -1.48189999e-02,\n",
       "         -1.65626665e-01,  -5.95399986e-02,  -2.20076666e-04,\n",
       "         -2.18866666e-01,   9.28100049e-02,  -4.92233336e-02,\n",
       "         -1.85590009e-01,   4.88599986e-02,  -3.16983337e-03,\n",
       "          3.88599994e-03,   3.40133334e-02,  -4.26400006e-02,\n",
       "         -2.81433324e-01,  -4.06033322e-02,  -5.60033321e-03,\n",
       "         -1.10929996e-01,  -5.17333349e-02,  -7.71033317e-02,\n",
       "         -6.39366657e-02,  -6.27433340e-01,  -2.55819996e-01,\n",
       "          3.30169996e-02,  -1.40416662e-01,  -6.50866677e-02,\n",
       "          1.33570004e+00,  -6.19799991e-02,  -1.74290001e-01,\n",
       "         -1.05603337e-01,   1.97376668e-04,   2.48163333e-03,\n",
       "          5.92600008e-02,  -5.29899995e-02,   4.01366657e-03,\n",
       "         -1.80743337e-02,  -9.95699962e-02,  -5.24966667e-02,\n",
       "         -1.15859995e-01,  -1.52123335e-02,  -1.47503336e-01,\n",
       "          6.26166662e-02,   9.28299967e-04,  -6.13700002e-02,\n",
       "         -3.83799995e-02,  -2.61936665e-01],\n",
       "       [  8.93933376e-02,   4.78200018e-02,  -9.29233332e-02,\n",
       "          5.41899974e-03,   3.79466663e-02,   2.33076672e-01,\n",
       "         -1.71106676e-01,  -1.57893330e-01,  -1.10249996e-01,\n",
       "         -4.61133321e-02,   9.00666714e-02,   1.03126665e-01,\n",
       "         -1.50040001e-01,  -1.37566666e-01,  -3.31066673e-02,\n",
       "          1.26949996e-02,   9.91633348e-03,   3.35866660e-02,\n",
       "         -8.35266709e-02,  -1.72726671e-01,   1.15193337e-01,\n",
       "          1.49740001e-01,   1.62636667e-01,  -2.69553338e-02,\n",
       "         -3.37366660e-02,  -4.59233324e-01,  -3.62199992e-02,\n",
       "         -7.73366690e-02,   4.27966658e-03,  -1.55026664e-01,\n",
       "          1.28209996e+00,   1.04540000e-01,   4.54766651e-02,\n",
       "         -1.74146672e-01,   1.10066662e-01,   1.12356663e-01,\n",
       "         -1.18669997e-01,   1.08103335e-01,   4.01366676e-02,\n",
       "          1.17066671e-01,  -2.30143343e-02,   1.22949998e-01,\n",
       "          8.38933289e-02,  -8.17233324e-02,   8.46033295e-02,\n",
       "          4.55666681e-02,  -1.03926669e-01,  -2.10699995e-01,\n",
       "         -8.34266643e-02,  -1.26990000e-01],\n",
       "       [  7.23500003e-02,   1.55050000e-01,  -1.55856669e-01,\n",
       "          3.36066658e-02,   3.37833325e-01,   2.49483327e-01,\n",
       "         -1.77013338e-01,  -8.75200033e-02,   5.60399989e-02,\n",
       "          4.39399978e-02,  -8.30300003e-02,  -1.47283336e-01,\n",
       "         -7.24633336e-02,   1.70013328e-01,   4.48266665e-02,\n",
       "         -1.43803338e-01,  -1.04100003e-02,   6.89133356e-02,\n",
       "         -2.60459999e-01,  -6.71600004e-02,  -3.24670002e-02,\n",
       "          5.36266665e-02,  -2.06119994e-01,  -6.16799990e-02,\n",
       "         -4.15366665e-02,  -7.50866652e-01,  -7.44033357e-02,\n",
       "          1.68099999e-01,   1.07523332e-01,   5.10433316e-02,\n",
       "          1.32119997e+00,  -2.37883329e-01,  -2.23373334e-01,\n",
       "          9.46266651e-02,   7.24600007e-02,   4.81099983e-02,\n",
       "          8.64199996e-02,   7.81133324e-02,   1.42466664e-01,\n",
       "         -1.48170004e-01,   4.60433314e-02,   1.23243332e-01,\n",
       "         -2.14296659e-01,   8.04733361e-03,  -1.31050001e-02,\n",
       "         -8.67899954e-02,   4.00566657e-02,  -1.45939998e-02,\n",
       "          1.36709998e-01,   5.98666668e-02],\n",
       "       [  2.36176670e-01,   1.90293332e-01,  -1.57199999e-01,\n",
       "          6.01600011e-02,   1.81496660e-01,   2.42009997e-01,\n",
       "          6.05233312e-02,  -1.74643338e-01,   3.46033325e-02,\n",
       "         -5.85533331e-02,   2.62839993e-02,  -1.20719999e-01,\n",
       "         -3.94299999e-02,  -2.77786672e-01,   3.97233342e-02,\n",
       "         -5.53500007e-02,   2.05183327e-02,  -4.23966659e-03,\n",
       "         -1.88743333e-01,   4.53866677e-03,   7.61700024e-02,\n",
       "         -4.79866664e-02,  -2.25163326e-02,  -1.27190004e-01,\n",
       "         -7.89933354e-02,  -5.67899982e-01,  -2.88973331e-01,\n",
       "         -8.90133381e-02,  -8.62999956e-02,   5.88999987e-02,\n",
       "          1.28919999e+00,  -5.37666678e-02,  -4.42433357e-02,\n",
       "         -2.29603330e-01,   6.14800006e-02,   1.74879997e-03,\n",
       "         -1.12913330e-01,  -2.63186668e-02,   8.06166679e-02,\n",
       "          1.21920000e-01,  -1.15756671e-01,   9.49433347e-02,\n",
       "          2.52309988e-02,  -2.07260003e-02,  -1.29960001e-01,\n",
       "          7.63399998e-02,  -7.20566660e-02,  -7.52066672e-02,\n",
       "         -3.13060010e-02,  -2.67916660e-01]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = create_emb()\n",
    "emb[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We pass our embedding matrix to the Embedding constructor, and set it to non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'the',\n",
       " 2: 'and',\n",
       " 3: 'a',\n",
       " 4: 'of',\n",
       " 5: 'to',\n",
       " 6: 'is',\n",
       " 7: 'br',\n",
       " 8: 'in',\n",
       " 9: 'it',\n",
       " 10: 'i',\n",
       " 11: 'this',\n",
       " 12: 'that',\n",
       " 13: 'was',\n",
       " 14: 'as',\n",
       " 15: 'for',\n",
       " 16: 'with',\n",
       " 17: 'movie',\n",
       " 18: 'but',\n",
       " 19: 'film',\n",
       " 20: 'on',\n",
       " 21: 'not',\n",
       " 22: 'you',\n",
       " 23: 'are',\n",
       " 24: 'his',\n",
       " 25: 'have',\n",
       " 26: 'he',\n",
       " 27: 'be',\n",
       " 28: 'one',\n",
       " 29: 'all',\n",
       " 30: 'at',\n",
       " 31: 'by',\n",
       " 32: 'an',\n",
       " 33: 'they',\n",
       " 34: 'who',\n",
       " 35: 'so',\n",
       " 36: 'from',\n",
       " 37: 'like',\n",
       " 38: 'her',\n",
       " 39: 'or',\n",
       " 40: 'just',\n",
       " 41: 'about',\n",
       " 42: \"it's\",\n",
       " 43: 'out',\n",
       " 44: 'has',\n",
       " 45: 'if',\n",
       " 46: 'some',\n",
       " 47: 'there',\n",
       " 48: 'what',\n",
       " 49: 'good',\n",
       " 50: 'more',\n",
       " 51: 'when',\n",
       " 52: 'very',\n",
       " 53: 'up',\n",
       " 54: 'no',\n",
       " 55: 'time',\n",
       " 56: 'she',\n",
       " 57: 'even',\n",
       " 58: 'my',\n",
       " 59: 'would',\n",
       " 60: 'which',\n",
       " 61: 'only',\n",
       " 62: 'story',\n",
       " 63: 'really',\n",
       " 64: 'see',\n",
       " 65: 'their',\n",
       " 66: 'had',\n",
       " 67: 'can',\n",
       " 68: 'were',\n",
       " 69: 'me',\n",
       " 70: 'well',\n",
       " 71: 'than',\n",
       " 72: 'we',\n",
       " 73: 'much',\n",
       " 74: 'been',\n",
       " 75: 'bad',\n",
       " 76: 'get',\n",
       " 77: 'will',\n",
       " 78: 'do',\n",
       " 79: 'also',\n",
       " 80: 'into',\n",
       " 81: 'people',\n",
       " 82: 'other',\n",
       " 83: 'first',\n",
       " 84: 'great',\n",
       " 85: 'because',\n",
       " 86: 'how',\n",
       " 87: 'him',\n",
       " 88: 'most',\n",
       " 89: \"don't\",\n",
       " 90: 'made',\n",
       " 91: 'its',\n",
       " 92: 'then',\n",
       " 93: 'way',\n",
       " 94: 'make',\n",
       " 95: 'them',\n",
       " 96: 'too',\n",
       " 97: 'could',\n",
       " 98: 'any',\n",
       " 99: 'movies',\n",
       " 100: 'after',\n",
       " 101: 'think',\n",
       " 102: 'characters',\n",
       " 103: 'watch',\n",
       " 104: 'two',\n",
       " 105: 'films',\n",
       " 106: 'character',\n",
       " 107: 'seen',\n",
       " 108: 'many',\n",
       " 109: 'being',\n",
       " 110: 'life',\n",
       " 111: 'plot',\n",
       " 112: 'never',\n",
       " 113: 'acting',\n",
       " 114: 'little',\n",
       " 115: 'best',\n",
       " 116: 'love',\n",
       " 117: 'over',\n",
       " 118: 'where',\n",
       " 119: 'did',\n",
       " 120: 'show',\n",
       " 121: 'know',\n",
       " 122: 'off',\n",
       " 123: 'ever',\n",
       " 124: 'does',\n",
       " 125: 'better',\n",
       " 126: 'your',\n",
       " 127: 'end',\n",
       " 128: 'still',\n",
       " 129: 'man',\n",
       " 130: 'here',\n",
       " 131: 'these',\n",
       " 132: 'say',\n",
       " 133: 'scene',\n",
       " 134: 'while',\n",
       " 135: 'why',\n",
       " 136: 'scenes',\n",
       " 137: 'go',\n",
       " 138: 'such',\n",
       " 139: 'something',\n",
       " 140: 'through',\n",
       " 141: 'should',\n",
       " 142: 'back',\n",
       " 143: \"i'm\",\n",
       " 144: 'real',\n",
       " 145: 'those',\n",
       " 146: 'watching',\n",
       " 147: 'now',\n",
       " 148: 'though',\n",
       " 149: \"doesn't\",\n",
       " 150: 'years',\n",
       " 151: 'old',\n",
       " 152: 'thing',\n",
       " 153: 'actors',\n",
       " 154: 'work',\n",
       " 155: '10',\n",
       " 156: 'before',\n",
       " 157: 'another',\n",
       " 158: \"didn't\",\n",
       " 159: 'new',\n",
       " 160: 'funny',\n",
       " 161: 'nothing',\n",
       " 162: 'actually',\n",
       " 163: 'makes',\n",
       " 164: 'director',\n",
       " 165: 'look',\n",
       " 166: 'find',\n",
       " 167: 'going',\n",
       " 168: 'few',\n",
       " 169: 'same',\n",
       " 170: 'part',\n",
       " 171: 'again',\n",
       " 172: 'every',\n",
       " 173: 'lot',\n",
       " 174: 'cast',\n",
       " 175: 'us',\n",
       " 176: 'quite',\n",
       " 177: 'down',\n",
       " 178: 'want',\n",
       " 179: 'world',\n",
       " 180: 'things',\n",
       " 181: 'pretty',\n",
       " 182: 'young',\n",
       " 183: 'seems',\n",
       " 184: 'around',\n",
       " 185: 'got',\n",
       " 186: 'horror',\n",
       " 187: 'however',\n",
       " 188: \"can't\",\n",
       " 189: 'fact',\n",
       " 190: 'take',\n",
       " 191: 'big',\n",
       " 192: 'enough',\n",
       " 193: 'long',\n",
       " 194: 'thought',\n",
       " 195: \"that's\",\n",
       " 196: 'both',\n",
       " 197: 'between',\n",
       " 198: 'series',\n",
       " 199: 'give',\n",
       " 200: 'may',\n",
       " 201: 'original',\n",
       " 202: 'own',\n",
       " 203: 'action',\n",
       " 204: \"i've\",\n",
       " 205: 'right',\n",
       " 206: 'without',\n",
       " 207: 'always',\n",
       " 208: 'times',\n",
       " 209: 'comedy',\n",
       " 210: 'point',\n",
       " 211: 'gets',\n",
       " 212: 'must',\n",
       " 213: 'come',\n",
       " 214: 'role',\n",
       " 215: \"isn't\",\n",
       " 216: 'saw',\n",
       " 217: 'almost',\n",
       " 218: 'interesting',\n",
       " 219: 'least',\n",
       " 220: 'family',\n",
       " 221: 'done',\n",
       " 222: \"there's\",\n",
       " 223: 'whole',\n",
       " 224: 'bit',\n",
       " 225: 'music',\n",
       " 226: 'script',\n",
       " 227: 'far',\n",
       " 228: 'making',\n",
       " 229: 'guy',\n",
       " 230: 'anything',\n",
       " 231: 'minutes',\n",
       " 232: 'feel',\n",
       " 233: 'last',\n",
       " 234: 'since',\n",
       " 235: 'might',\n",
       " 236: 'performance',\n",
       " 237: \"he's\",\n",
       " 238: '2',\n",
       " 239: 'probably',\n",
       " 240: 'kind',\n",
       " 241: 'am',\n",
       " 242: 'away',\n",
       " 243: 'yet',\n",
       " 244: 'rather',\n",
       " 245: 'tv',\n",
       " 246: 'worst',\n",
       " 247: 'girl',\n",
       " 248: 'day',\n",
       " 249: 'sure',\n",
       " 250: 'fun',\n",
       " 251: 'hard',\n",
       " 252: 'woman',\n",
       " 253: 'played',\n",
       " 254: 'each',\n",
       " 255: 'found',\n",
       " 256: 'anyone',\n",
       " 257: 'having',\n",
       " 258: 'although',\n",
       " 259: 'especially',\n",
       " 260: 'our',\n",
       " 261: 'believe',\n",
       " 262: 'course',\n",
       " 263: 'comes',\n",
       " 264: 'looking',\n",
       " 265: 'screen',\n",
       " 266: 'trying',\n",
       " 267: 'set',\n",
       " 268: 'goes',\n",
       " 269: 'looks',\n",
       " 270: 'place',\n",
       " 271: 'book',\n",
       " 272: 'different',\n",
       " 273: 'put',\n",
       " 274: 'ending',\n",
       " 275: 'money',\n",
       " 276: 'maybe',\n",
       " 277: 'once',\n",
       " 278: 'sense',\n",
       " 279: 'reason',\n",
       " 280: 'true',\n",
       " 281: 'actor',\n",
       " 282: 'everything',\n",
       " 283: \"wasn't\",\n",
       " 284: 'shows',\n",
       " 285: 'dvd',\n",
       " 286: 'three',\n",
       " 287: 'worth',\n",
       " 288: 'year',\n",
       " 289: 'job',\n",
       " 290: 'main',\n",
       " 291: 'someone',\n",
       " 292: 'together',\n",
       " 293: 'watched',\n",
       " 294: 'play',\n",
       " 295: 'american',\n",
       " 296: 'plays',\n",
       " 297: '1',\n",
       " 298: 'said',\n",
       " 299: 'effects',\n",
       " 300: 'later',\n",
       " 301: 'takes',\n",
       " 302: 'instead',\n",
       " 303: 'seem',\n",
       " 304: 'beautiful',\n",
       " 305: 'john',\n",
       " 306: 'himself',\n",
       " 307: 'version',\n",
       " 308: 'audience',\n",
       " 309: 'high',\n",
       " 310: 'house',\n",
       " 311: 'night',\n",
       " 312: 'during',\n",
       " 313: 'everyone',\n",
       " 314: 'left',\n",
       " 315: 'special',\n",
       " 316: 'seeing',\n",
       " 317: 'half',\n",
       " 318: 'excellent',\n",
       " 319: 'wife',\n",
       " 320: 'star',\n",
       " 321: 'shot',\n",
       " 322: 'war',\n",
       " 323: 'idea',\n",
       " 324: 'nice',\n",
       " 325: 'black',\n",
       " 326: 'less',\n",
       " 327: 'mind',\n",
       " 328: 'simply',\n",
       " 329: 'read',\n",
       " 330: 'second',\n",
       " 331: 'else',\n",
       " 332: \"you're\",\n",
       " 333: 'father',\n",
       " 334: 'fan',\n",
       " 335: 'poor',\n",
       " 336: 'help',\n",
       " 337: 'completely',\n",
       " 338: 'death',\n",
       " 339: '3',\n",
       " 340: 'used',\n",
       " 341: 'home',\n",
       " 342: 'either',\n",
       " 343: 'short',\n",
       " 344: 'line',\n",
       " 345: 'given',\n",
       " 346: 'men',\n",
       " 347: 'top',\n",
       " 348: 'dead',\n",
       " 349: 'budget',\n",
       " 350: 'try',\n",
       " 351: 'performances',\n",
       " 352: 'wrong',\n",
       " 353: 'classic',\n",
       " 354: 'boring',\n",
       " 355: 'enjoy',\n",
       " 356: 'need',\n",
       " 357: 'rest',\n",
       " 358: 'use',\n",
       " 359: 'kids',\n",
       " 360: 'hollywood',\n",
       " 361: 'low',\n",
       " 362: 'production',\n",
       " 363: 'until',\n",
       " 364: 'along',\n",
       " 365: 'full',\n",
       " 366: 'friends',\n",
       " 367: 'camera',\n",
       " 368: 'truly',\n",
       " 369: 'women',\n",
       " 370: 'awful',\n",
       " 371: 'video',\n",
       " 372: 'next',\n",
       " 373: 'tell',\n",
       " 374: 'remember',\n",
       " 375: 'couple',\n",
       " 376: 'stupid',\n",
       " 377: 'start',\n",
       " 378: 'stars',\n",
       " 379: 'perhaps',\n",
       " 380: 'sex',\n",
       " 381: 'mean',\n",
       " 382: 'came',\n",
       " 383: 'recommend',\n",
       " 384: 'let',\n",
       " 385: 'moments',\n",
       " 386: 'wonderful',\n",
       " 387: 'episode',\n",
       " 388: 'understand',\n",
       " 389: 'small',\n",
       " 390: 'face',\n",
       " 391: 'terrible',\n",
       " 392: 'playing',\n",
       " 393: 'school',\n",
       " 394: 'getting',\n",
       " 395: 'written',\n",
       " 396: 'doing',\n",
       " 397: 'often',\n",
       " 398: 'keep',\n",
       " 399: 'early',\n",
       " 400: 'name',\n",
       " 401: 'perfect',\n",
       " 402: 'style',\n",
       " 403: 'human',\n",
       " 404: 'definitely',\n",
       " 405: 'gives',\n",
       " 406: 'others',\n",
       " 407: 'itself',\n",
       " 408: 'lines',\n",
       " 409: 'live',\n",
       " 410: 'become',\n",
       " 411: 'dialogue',\n",
       " 412: 'person',\n",
       " 413: 'lost',\n",
       " 414: 'finally',\n",
       " 415: 'piece',\n",
       " 416: 'head',\n",
       " 417: 'case',\n",
       " 418: 'felt',\n",
       " 419: 'yes',\n",
       " 420: 'liked',\n",
       " 421: 'supposed',\n",
       " 422: 'title',\n",
       " 423: \"couldn't\",\n",
       " 424: 'absolutely',\n",
       " 425: 'white',\n",
       " 426: 'against',\n",
       " 427: 'boy',\n",
       " 428: 'picture',\n",
       " 429: 'sort',\n",
       " 430: 'worse',\n",
       " 431: 'certainly',\n",
       " 432: 'went',\n",
       " 433: 'entire',\n",
       " 434: 'waste',\n",
       " 435: 'cinema',\n",
       " 436: 'problem',\n",
       " 437: 'hope',\n",
       " 438: 'entertaining',\n",
       " 439: \"she's\",\n",
       " 440: 'mr',\n",
       " 441: 'overall',\n",
       " 442: 'evil',\n",
       " 443: 'called',\n",
       " 444: 'loved',\n",
       " 445: 'based',\n",
       " 446: 'oh',\n",
       " 447: 'several',\n",
       " 448: 'fans',\n",
       " 449: 'mother',\n",
       " 450: 'drama',\n",
       " 451: 'beginning',\n",
       " 452: 'killer',\n",
       " 453: 'lives',\n",
       " 454: '5',\n",
       " 455: 'direction',\n",
       " 456: 'care',\n",
       " 457: 'already',\n",
       " 458: 'becomes',\n",
       " 459: 'laugh',\n",
       " 460: 'example',\n",
       " 461: 'friend',\n",
       " 462: 'dark',\n",
       " 463: 'despite',\n",
       " 464: 'under',\n",
       " 465: 'seemed',\n",
       " 466: 'throughout',\n",
       " 467: '4',\n",
       " 468: 'turn',\n",
       " 469: 'unfortunately',\n",
       " 470: 'wanted',\n",
       " 471: \"i'd\",\n",
       " 472: '\\xc2\\x96',\n",
       " 473: 'children',\n",
       " 474: 'final',\n",
       " 475: 'fine',\n",
       " 476: 'history',\n",
       " 477: 'amazing',\n",
       " 478: 'sound',\n",
       " 479: 'guess',\n",
       " 480: 'heart',\n",
       " 481: 'totally',\n",
       " 482: 'lead',\n",
       " 483: 'humor',\n",
       " 484: 'writing',\n",
       " 485: 'michael',\n",
       " 486: 'quality',\n",
       " 487: \"you'll\",\n",
       " 488: 'close',\n",
       " 489: 'son',\n",
       " 490: 'guys',\n",
       " 491: 'wants',\n",
       " 492: 'works',\n",
       " 493: 'behind',\n",
       " 494: 'tries',\n",
       " 495: 'art',\n",
       " 496: 'side',\n",
       " 497: 'game',\n",
       " 498: 'past',\n",
       " 499: 'able',\n",
       " 500: 'b',\n",
       " 501: 'days',\n",
       " 502: 'turns',\n",
       " 503: 'child',\n",
       " 504: \"they're\",\n",
       " 505: 'hand',\n",
       " 506: 'flick',\n",
       " 507: 'enjoyed',\n",
       " 508: 'act',\n",
       " 509: 'genre',\n",
       " 510: 'town',\n",
       " 511: 'favorite',\n",
       " 512: 'soon',\n",
       " 513: 'kill',\n",
       " 514: 'starts',\n",
       " 515: 'sometimes',\n",
       " 516: 'car',\n",
       " 517: 'gave',\n",
       " 518: 'run',\n",
       " 519: 'late',\n",
       " 520: 'eyes',\n",
       " 521: 'actress',\n",
       " 522: 'etc',\n",
       " 523: 'directed',\n",
       " 524: 'horrible',\n",
       " 525: \"won't\",\n",
       " 526: 'viewer',\n",
       " 527: 'brilliant',\n",
       " 528: 'parts',\n",
       " 529: 'self',\n",
       " 530: 'themselves',\n",
       " 531: 'hour',\n",
       " 532: 'expect',\n",
       " 533: 'thinking',\n",
       " 534: 'stories',\n",
       " 535: 'stuff',\n",
       " 536: 'girls',\n",
       " 537: 'obviously',\n",
       " 538: 'blood',\n",
       " 539: 'decent',\n",
       " 540: 'city',\n",
       " 541: 'voice',\n",
       " 542: 'highly',\n",
       " 543: 'myself',\n",
       " 544: 'feeling',\n",
       " 545: 'fight',\n",
       " 546: 'except',\n",
       " 547: 'slow',\n",
       " 548: 'matter',\n",
       " 549: 'type',\n",
       " 550: 'anyway',\n",
       " 551: 'kid',\n",
       " 552: 'roles',\n",
       " 553: 'killed',\n",
       " 554: 'heard',\n",
       " 555: 'god',\n",
       " 556: 'age',\n",
       " 557: 'says',\n",
       " 558: 'moment',\n",
       " 559: 'took',\n",
       " 560: 'leave',\n",
       " 561: 'writer',\n",
       " 562: 'strong',\n",
       " 563: 'cannot',\n",
       " 564: 'violence',\n",
       " 565: 'police',\n",
       " 566: 'hit',\n",
       " 567: 'stop',\n",
       " 568: 'happens',\n",
       " 569: 'particularly',\n",
       " 570: 'known',\n",
       " 571: 'involved',\n",
       " 572: 'happened',\n",
       " 573: 'extremely',\n",
       " 574: 'daughter',\n",
       " 575: 'obvious',\n",
       " 576: 'told',\n",
       " 577: 'chance',\n",
       " 578: 'living',\n",
       " 579: 'coming',\n",
       " 580: 'lack',\n",
       " 581: 'alone',\n",
       " 582: 'experience',\n",
       " 583: \"wouldn't\",\n",
       " 584: 'including',\n",
       " 585: 'murder',\n",
       " 586: 'attempt',\n",
       " 587: 's',\n",
       " 588: 'please',\n",
       " 589: 'james',\n",
       " 590: 'happen',\n",
       " 591: 'wonder',\n",
       " 592: 'crap',\n",
       " 593: 'ago',\n",
       " 594: 'brother',\n",
       " 595: \"film's\",\n",
       " 596: 'gore',\n",
       " 597: 'none',\n",
       " 598: 'complete',\n",
       " 599: 'interest',\n",
       " 600: 'score',\n",
       " 601: 'group',\n",
       " 602: 'cut',\n",
       " 603: 'simple',\n",
       " 604: 'save',\n",
       " 605: 'ok',\n",
       " 606: 'hell',\n",
       " 607: 'looked',\n",
       " 608: 'career',\n",
       " 609: 'number',\n",
       " 610: 'song',\n",
       " 611: 'possible',\n",
       " 612: 'seriously',\n",
       " 613: 'annoying',\n",
       " 614: 'shown',\n",
       " 615: 'exactly',\n",
       " 616: 'sad',\n",
       " 617: 'running',\n",
       " 618: 'musical',\n",
       " 619: 'serious',\n",
       " 620: 'taken',\n",
       " 621: 'yourself',\n",
       " 622: 'whose',\n",
       " 623: 'released',\n",
       " 624: 'cinematography',\n",
       " 625: 'david',\n",
       " 626: 'scary',\n",
       " 627: 'ends',\n",
       " 628: 'english',\n",
       " 629: 'hero',\n",
       " 630: 'usually',\n",
       " 631: 'hours',\n",
       " 632: 'reality',\n",
       " 633: 'opening',\n",
       " 634: \"i'll\",\n",
       " 635: 'across',\n",
       " 636: 'today',\n",
       " 637: 'jokes',\n",
       " 638: 'light',\n",
       " 639: 'hilarious',\n",
       " 640: 'somewhat',\n",
       " 641: 'usual',\n",
       " 642: 'started',\n",
       " 643: 'cool',\n",
       " 644: 'ridiculous',\n",
       " 645: 'body',\n",
       " 646: 'relationship',\n",
       " 647: 'view',\n",
       " 648: 'level',\n",
       " 649: 'opinion',\n",
       " 650: 'change',\n",
       " 651: 'happy',\n",
       " 652: 'middle',\n",
       " 653: 'taking',\n",
       " 654: 'wish',\n",
       " 655: 'husband',\n",
       " 656: 'finds',\n",
       " 657: 'saying',\n",
       " 658: 'order',\n",
       " 659: 'talking',\n",
       " 660: 'ones',\n",
       " 661: 'documentary',\n",
       " 662: 'shots',\n",
       " 663: 'huge',\n",
       " 664: 'novel',\n",
       " 665: 'female',\n",
       " 666: 'mostly',\n",
       " 667: 'robert',\n",
       " 668: 'power',\n",
       " 669: 'episodes',\n",
       " 670: 'room',\n",
       " 671: 'important',\n",
       " 672: 'rating',\n",
       " 673: 'talent',\n",
       " 674: 'five',\n",
       " 675: 'major',\n",
       " 676: 'turned',\n",
       " 677: 'strange',\n",
       " 678: 'word',\n",
       " 679: 'modern',\n",
       " 680: 'call',\n",
       " 681: 'apparently',\n",
       " 682: 'disappointed',\n",
       " 683: 'single',\n",
       " 684: 'events',\n",
       " 685: 'due',\n",
       " 686: 'four',\n",
       " 687: 'songs',\n",
       " 688: 'basically',\n",
       " 689: 'attention',\n",
       " 690: '7',\n",
       " 691: 'knows',\n",
       " 692: 'clearly',\n",
       " 693: 'supporting',\n",
       " 694: 'knew',\n",
       " 695: 'british',\n",
       " 696: 'television',\n",
       " 697: 'comic',\n",
       " 698: 'non',\n",
       " 699: 'fast',\n",
       " 700: 'earth',\n",
       " 701: 'country',\n",
       " 702: 'future',\n",
       " 703: 'cheap',\n",
       " 704: 'class',\n",
       " 705: 'thriller',\n",
       " 706: '8',\n",
       " 707: 'silly',\n",
       " 708: 'king',\n",
       " 709: 'problems',\n",
       " 710: \"aren't\",\n",
       " 711: 'easily',\n",
       " 712: 'words',\n",
       " 713: 'tells',\n",
       " 714: 'miss',\n",
       " 715: 'jack',\n",
       " 716: 'local',\n",
       " 717: 'sequence',\n",
       " 718: 'bring',\n",
       " 719: 'entertainment',\n",
       " 720: 'paul',\n",
       " 721: 'beyond',\n",
       " 722: 'upon',\n",
       " 723: 'whether',\n",
       " 724: 'predictable',\n",
       " 725: 'moving',\n",
       " 726: 'similar',\n",
       " 727: 'straight',\n",
       " 728: 'romantic',\n",
       " 729: 'sets',\n",
       " 730: 'review',\n",
       " 731: 'falls',\n",
       " 732: 'oscar',\n",
       " 733: 'mystery',\n",
       " 734: 'enjoyable',\n",
       " 735: 'needs',\n",
       " 736: 'appears',\n",
       " 737: 'talk',\n",
       " 738: 'rock',\n",
       " 739: 'george',\n",
       " 740: 'giving',\n",
       " 741: 'eye',\n",
       " 742: 'richard',\n",
       " 743: 'within',\n",
       " 744: 'ten',\n",
       " 745: 'animation',\n",
       " 746: 'message',\n",
       " 747: 'theater',\n",
       " 748: 'near',\n",
       " 749: 'above',\n",
       " 750: 'dull',\n",
       " 751: 'nearly',\n",
       " 752: 'sequel',\n",
       " 753: 'theme',\n",
       " 754: 'points',\n",
       " 755: \"'\",\n",
       " 756: 'stand',\n",
       " 757: 'mention',\n",
       " 758: 'lady',\n",
       " 759: 'bunch',\n",
       " 760: 'add',\n",
       " 761: 'feels',\n",
       " 762: 'herself',\n",
       " 763: 'release',\n",
       " 764: 'red',\n",
       " 765: 'team',\n",
       " 766: 'storyline',\n",
       " 767: 'surprised',\n",
       " 768: 'ways',\n",
       " 769: 'using',\n",
       " 770: 'named',\n",
       " 771: \"haven't\",\n",
       " 772: 'lots',\n",
       " 773: 'easy',\n",
       " 774: 'fantastic',\n",
       " 775: 'begins',\n",
       " 776: 'actual',\n",
       " 777: 'working',\n",
       " 778: 'effort',\n",
       " 779: 'york',\n",
       " 780: 'die',\n",
       " 781: 'hate',\n",
       " 782: 'french',\n",
       " 783: 'minute',\n",
       " 784: 'tale',\n",
       " 785: 'clear',\n",
       " 786: 'stay',\n",
       " 787: '9',\n",
       " 788: 'elements',\n",
       " 789: 'feature',\n",
       " 790: 'among',\n",
       " 791: 'follow',\n",
       " 792: 'comments',\n",
       " 793: 're',\n",
       " 794: 'viewers',\n",
       " 795: 'avoid',\n",
       " 796: 'sister',\n",
       " 797: 'showing',\n",
       " 798: 'typical',\n",
       " 799: 'editing',\n",
       " 800: \"what's\",\n",
       " 801: 'famous',\n",
       " 802: 'tried',\n",
       " 803: 'sorry',\n",
       " 804: 'dialog',\n",
       " 805: 'check',\n",
       " 806: 'fall',\n",
       " 807: 'period',\n",
       " 808: 'season',\n",
       " 809: 'form',\n",
       " 810: 'certain',\n",
       " 811: 'filmed',\n",
       " 812: 'weak',\n",
       " 813: 'soundtrack',\n",
       " 814: 'means',\n",
       " 815: 'buy',\n",
       " 816: 'material',\n",
       " 817: 'somehow',\n",
       " 818: 'realistic',\n",
       " 819: 'figure',\n",
       " 820: 'crime',\n",
       " 821: 'doubt',\n",
       " 822: 'gone',\n",
       " 823: 'peter',\n",
       " 824: 'tom',\n",
       " 825: 'kept',\n",
       " 826: 'viewing',\n",
       " 827: 't',\n",
       " 828: 'general',\n",
       " 829: 'leads',\n",
       " 830: 'greatest',\n",
       " 831: 'space',\n",
       " 832: 'lame',\n",
       " 833: 'suspense',\n",
       " 834: 'dance',\n",
       " 835: 'imagine',\n",
       " 836: 'brought',\n",
       " 837: 'third',\n",
       " 838: 'atmosphere',\n",
       " 839: 'hear',\n",
       " 840: 'particular',\n",
       " 841: 'sequences',\n",
       " 842: 'whatever',\n",
       " 843: 'parents',\n",
       " 844: 'move',\n",
       " 845: 'lee',\n",
       " 846: 'indeed',\n",
       " 847: 'learn',\n",
       " 848: 'rent',\n",
       " 849: 'de',\n",
       " 850: 'eventually',\n",
       " 851: 'note',\n",
       " 852: 'deal',\n",
       " 853: 'average',\n",
       " 854: 'reviews',\n",
       " 855: 'wait',\n",
       " 856: 'forget',\n",
       " 857: 'japanese',\n",
       " 858: 'sexual',\n",
       " 859: 'poorly',\n",
       " 860: 'premise',\n",
       " 861: 'okay',\n",
       " 862: 'zombie',\n",
       " 863: 'surprise',\n",
       " 864: 'believable',\n",
       " 865: 'stage',\n",
       " 866: 'possibly',\n",
       " 867: 'sit',\n",
       " 868: \"who's\",\n",
       " 869: 'decided',\n",
       " 870: 'expected',\n",
       " 871: \"you've\",\n",
       " 872: 'subject',\n",
       " 873: 'nature',\n",
       " 874: 'became',\n",
       " 875: 'difficult',\n",
       " 876: 'free',\n",
       " 877: 'killing',\n",
       " 878: 'screenplay',\n",
       " 879: 'truth',\n",
       " 880: 'romance',\n",
       " 881: 'dr',\n",
       " 882: 'nor',\n",
       " 883: 'reading',\n",
       " 884: 'needed',\n",
       " 885: 'question',\n",
       " 886: 'leaves',\n",
       " 887: 'street',\n",
       " 888: '20',\n",
       " 889: 'meets',\n",
       " 890: 'hot',\n",
       " 891: 'unless',\n",
       " 892: 'begin',\n",
       " 893: 'baby',\n",
       " 894: 'superb',\n",
       " 895: 'credits',\n",
       " 896: 'imdb',\n",
       " 897: 'otherwise',\n",
       " 898: 'write',\n",
       " 899: 'shame',\n",
       " 900: \"let's\",\n",
       " 901: 'situation',\n",
       " 902: 'dramatic',\n",
       " 903: 'memorable',\n",
       " 904: 'directors',\n",
       " 905: 'earlier',\n",
       " 906: 'meet',\n",
       " 907: 'disney',\n",
       " 908: 'open',\n",
       " 909: 'dog',\n",
       " 910: 'badly',\n",
       " 911: 'joe',\n",
       " 912: 'male',\n",
       " 913: 'weird',\n",
       " 914: 'acted',\n",
       " 915: 'forced',\n",
       " 916: 'laughs',\n",
       " 917: 'sci',\n",
       " 918: 'emotional',\n",
       " 919: 'older',\n",
       " 920: 'realize',\n",
       " 921: 'fi',\n",
       " 922: 'dream',\n",
       " 923: 'society',\n",
       " 924: 'writers',\n",
       " 925: 'interested',\n",
       " 926: 'footage',\n",
       " 927: 'forward',\n",
       " 928: 'comment',\n",
       " 929: 'crazy',\n",
       " 930: 'deep',\n",
       " 931: 'sounds',\n",
       " 932: 'plus',\n",
       " 933: 'beauty',\n",
       " 934: 'whom',\n",
       " 935: 'america',\n",
       " 936: 'fantasy',\n",
       " 937: 'directing',\n",
       " 938: 'keeps',\n",
       " 939: 'ask',\n",
       " 940: 'development',\n",
       " 941: 'features',\n",
       " 942: 'air',\n",
       " 943: 'quickly',\n",
       " 944: 'mess',\n",
       " 945: 'creepy',\n",
       " 946: 'towards',\n",
       " 947: 'perfectly',\n",
       " 948: 'mark',\n",
       " 949: 'worked',\n",
       " 950: 'box',\n",
       " 951: 'cheesy',\n",
       " 952: 'unique',\n",
       " 953: 'setting',\n",
       " 954: 'hands',\n",
       " 955: 'plenty',\n",
       " 956: 'result',\n",
       " 957: 'previous',\n",
       " 958: 'brings',\n",
       " 959: 'effect',\n",
       " 960: 'e',\n",
       " 961: 'total',\n",
       " 962: 'personal',\n",
       " 963: 'incredibly',\n",
       " 964: 'rate',\n",
       " 965: 'fire',\n",
       " 966: 'monster',\n",
       " 967: 'business',\n",
       " 968: 'leading',\n",
       " 969: 'apart',\n",
       " 970: 'casting',\n",
       " 971: 'admit',\n",
       " 972: 'joke',\n",
       " 973: 'powerful',\n",
       " 974: 'appear',\n",
       " 975: 'background',\n",
       " 976: 'telling',\n",
       " 977: 'girlfriend',\n",
       " 978: 'meant',\n",
       " 979: 'christmas',\n",
       " 980: 'hardly',\n",
       " 981: 'present',\n",
       " 982: 'battle',\n",
       " 983: 'potential',\n",
       " 984: 'create',\n",
       " 985: 'bill',\n",
       " 986: 'break',\n",
       " 987: 'pay',\n",
       " 988: 'masterpiece',\n",
       " 989: 'gay',\n",
       " 990: 'political',\n",
       " 991: 'return',\n",
       " 992: 'dumb',\n",
       " 993: 'fails',\n",
       " 994: 'fighting',\n",
       " 995: 'various',\n",
       " 996: 'era',\n",
       " 997: 'portrayed',\n",
       " 998: 'co',\n",
       " 999: 'cop',\n",
       " 1000: 'secret',\n",
       " ...}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, \n",
    "              weights=[emb], trainable=True),\n",
    "#    Dropout(0.25),\n",
    "    Convolution1D(64, 5, border_mode='same', activation='relu'),\n",
    "#    Dropout(0.25),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "#    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 97s - loss: 0.4261 - acc: 0.7870 - val_loss: 0.2837 - val_acc: 0.8836\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 105s - loss: 0.2626 - acc: 0.8912 - val_loss: 0.2514 - val_acc: 0.8983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f3c900fd0>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We already have beaten our previous model! But let's fine-tune the embedding weights - especially since the words we couldn't find in glove just have random embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 190s - loss: 0.4704 - acc: 0.7824 - val_loss: 0.4405 - val_acc: 0.8004\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 188s - loss: 0.4610 - acc: 0.7864 - val_loss: 0.4807 - val_acc: 0.7616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f60185ed0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As expected, that's given us a nice little boost. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'glove50.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Multi-size CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is an implementation of a multi-size CNN as shown in Ben Bowles' [excellent blog post](https://quid.com/feed/how-quid-uses-deep-learning-with-small-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use the functional API to create multiple conv layers of different sizes, and then concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "graph_in = Input ((vocab_size, 50))\n",
    "convs = [ ] \n",
    "for fsz in range (3, 6): \n",
    "    x = Convolution1D(64, fsz, border_mode='same', activation=\"relu\")(graph_in)\n",
    "    x = MaxPooling1D()(x) \n",
    "    x = Flatten()(x) \n",
    "    convs.append(x)\n",
    "out = Merge(mode=\"concat\")(convs) \n",
    "graph = Model(graph_in, out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We then replace the conv/max-pool layer in our original CNN with the concatenated conv layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential ([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, weights=[emb]),\n",
    "    Dropout (0.2),\n",
    "    graph,\n",
    "    Dropout (0.5),\n",
    "    Dense (100, activation=\"relu\"),\n",
    "    Dropout (0.7),\n",
    "    Dense (1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 11s - loss: 0.3997 - acc: 0.8207 - val_loss: 0.3032 - val_acc: 0.8943\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 11s - loss: 0.2882 - acc: 0.8832 - val_loss: 0.2646 - val_acc: 0.9029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f55b79b7990>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Interestingly, I found that in this case I got best results when I started the embedding layer as being trainable, and then set it to non-trainable after a couple of epochs. I have no idea why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 11s - loss: 0.2556 - acc: 0.8949 - val_loss: 0.2534 - val_acc: 0.9024\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 11s - loss: 0.2360 - acc: 0.9057 - val_loss: 0.2577 - val_acc: 0.9036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f55b74de110>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This more complex architecture has given us another boost in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We haven't covered this bit yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_8 (Embedding)          (None, 500, 32)       160000      embedding_input_6[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 100)           53200       embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 1)             101         lstm_1[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len, mask_zero=True,\n",
    "              W_regularizer=l2(1e-6), dropout=0.2),\n",
    "    LSTM(100, consume_less='gpu'),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-885a65551b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-1.2.2-py2.7.egg/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-1.2.2-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1137\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 batch_size=batch_size)\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m             \u001b[0mval_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-1.2.2-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_make_test_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    782\u001b[0m                                             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m                                             \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_updates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m                                             **self._function_kwargs)\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-1.2.2-py2.7.egg/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Invalid argument \"%s\" passed to K.function'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-1.2.2-py2.7.egg/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m                                         \u001b[0mallow_input_downcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                                         \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                                         **kwargs)\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Theano-0.9.0-py2.7.egg/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    324\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# borrowed used defined inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Theano-0.9.0-py2.7.egg/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Theano-0.9.0-py2.7.egg/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1793\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                    \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1795\u001b[0;31m             defaults)\n\u001b[0m\u001b[1;32m   1796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Theano-0.9.0-py2.7.egg/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input_storage, trustme, storage_map)\u001b[0m\n\u001b[1;32m   1659\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m             _fn, _i, _o = self.linker.make_thunk(\n\u001b[0;32m-> 1661\u001b[0;31m                 input_storage=input_storage_lists, storage_map=storage_map)\n\u001b[0m\u001b[1;32m   1662\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimit_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Theano-0.9.0-py2.7.egg/theano/gof/link.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m    697\u001b[0m         return self.make_all(input_storage=input_storage,\n\u001b[1;32m    698\u001b[0m                              \u001b[0moutput_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                              storage_map=storage_map)[:3]\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Theano-0.9.0-py2.7.egg/theano/gof/vm.pyc\u001b[0m in \u001b[0;36mmake_all\u001b[0;34m(self, profiler, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                                                  \u001b[0mcompute_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                                                  \u001b[0mno_recycling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                                                  impl=impl))\n\u001b[0m\u001b[1;32m   1048\u001b[0m                 \u001b[0mlinker_make_thunk_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mthunk_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lazy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Theano-0.9.0-py2.7.egg/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling, impl)\u001b[0m\n\u001b[1;32m    949\u001b[0m             cython_destroy_map = numpy.asarray(cython_destroy_map,\n\u001b[1;32m    950\u001b[0m                                                dtype='int32')\n\u001b[0;32m--> 951\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_perform_ext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Theano-0.9.0-py2.7.egg/theano/scan_module/scan_perform_ext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m             cmodule.GCC_compiler.compile_str(dirname, code, location=loc,\n\u001b[1;32m    121\u001b[0m                                              \u001b[0mpreargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                                              hide_symbols=False)\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;31m# Save version into the __init__.py file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0minit_py\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__init__.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Theano-0.9.0-py2.7.egg/theano/gof/cmodule.pyc\u001b[0m in \u001b[0;36mcompile_str\u001b[0;34m(module_name, src_code, location, include_dirs, lib_dirs, libs, preargs, py_module, hide_symbols)\u001b[0m\n\u001b[1;32m   2292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2294\u001b[0;31m             \u001b[0mp_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_subprocess_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2295\u001b[0m             \u001b[0mcompile_stderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2296\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Theano-0.9.0-py2.7.egg/theano/misc/windows.pyc\u001b[0m in \u001b[0;36moutput_subprocess_Popen\u001b[0;34m(command, **params)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# we need to use communicate to make sure we don't deadlock around\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# the stdout/stderr pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_has_poll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1417\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_with_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1418\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_with_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_communicate_with_poll\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1469\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mfd2file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
